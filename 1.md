# Лабораторная работа №1. Знакомство с OpenAI API. Реализация простого текстового ассистента

**Цель работы:** изучить принципы взаимодействия с OpenAI API (на примере OpenAI-совместимого HTTP-интерфейса) и реализовать консольного текстового ассистента на Python, поддерживающего системный промпт, управление параметром генерации `temperature` и ведение истории диалога с ограничением длины контекста.

---

## План работы

1. Настройка окружения  
2. Обращение к OpenAI Responses API (через OpenAI-совместимый endpoint)  
3. Реализация главного цикла приложения  
4. Troubleshooting  
5. Выполнение заданий и анализ результатов  

---

## 1. Реализация приложения

Ниже представлен полный исходный код консольного текстового ассистента, реализованного на языке Python.  
Реализованы следующие функции:
- загрузка ключа доступа и системного промпта из переменных окружения (`.env`);
- формирование и отправка запроса к API языковой модели;
- хранение системных промптов в базе данных SQLite;
- ведение истории диалога (контекста переписки) с ограничением до 6 последних сообщений (3 пользовательских и 3 сообщений ассистента);
- консольные команды для управления промптами, параметром `temperature` и просмотром истории.

### 1.1 Полный исходный код

```python
import os
import sqlite3
import requests
from dotenv import load_dotenv

load_dotenv()

HF_API_KEY = os.getenv("HF_API_KEY")
SYSTEM_PROMPT = os.getenv(
    "SYSTEM_PROMPT",
    "Ты вежливая и хорошая булочка."
)

BASE_URL = "https://router.huggingface.co/v1/chat/completions"
MODEL_NAME = "openai/gpt-oss-120b"

DB_NAME = "prompts.db"
MAX_MEMORY = 6
temperature = 0.7


def get_db():
    return sqlite3.connect(DB_NAME)


def init_db():
    with get_db() as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS prompts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE NOT NULL,
                content TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        conn.execute("""
            CREATE TABLE IF NOT EXISTS chat_memory (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)


def save_prompt(name: str, content: str) -> bool:
    try:
        with get_db() as conn:
            conn.execute(
                "INSERT INTO prompts (name, content) VALUES (?, ?)",
                (name, content)
            )
        return True
    except sqlite3.IntegrityError:
        return False


def list_prompts():
    with get_db() as conn:
        return conn.execute(
            "SELECT id, name, content FROM prompts ORDER BY created_at DESC"
        ).fetchall()


def load_prompt(prompt_id: int):
    with get_db() as conn:
        row = conn.execute(
            "SELECT content FROM prompts WHERE id = ?",
            (prompt_id,)
        ).fetchone()
    return row[0] if row else None


def delete_prompt(prompt_id: int):
    with get_db() as conn:
        conn.execute(
            "DELETE FROM prompts WHERE id = ?",
            (prompt_id,)
        )


def save_message(role: str, content: str):
    with get_db() as conn:
        conn.execute(
            "INSERT INTO chat_memory (role, content) VALUES (?, ?)",
            (role, content)
        )

        conn.execute(f"""
            DELETE FROM chat_memory
            WHERE id NOT IN (
                SELECT id FROM chat_memory
                ORDER BY created_at DESC
                LIMIT {MAX_MEMORY}
            )
        """)


def load_memory():
    with get_db() as conn:
        rows = conn.execute("""
            SELECT role, content
            FROM chat_memory
            ORDER BY created_at ASC
        """).fetchall()

    return [{"role": r, "content": c} for r, c in rows]


def clear_memory():
    with get_db() as conn:
        conn.execute("DELETE FROM chat_memory")


def print_history():
    memory = load_memory()
    if not memory:
        print("История пуста.")
        return

    print("\n=== История диалога ===")
    for msg in memory:
        role = "USER" if msg["role"] == "user" else "AI"
        print(f"{role}: {msg['content']}")
    print("======================\n")


def call_llm(messages, temp: float):
    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": temp
    }

    headers = {
        "Authorization": f"Bearer {HF_API_KEY}",
        "Content-Type": "application/json"
    }

    r = requests.post(BASE_URL, json=payload, headers=headers)

    if r.status_code != 200:
        return f"Ошибка API: {r.status_code} {r.text}"

    try:
        return r.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Ошибка парсинга ответа: {e}"


def print_help():
    print("""
Команды:
  exit                 — выход (память очищается)
  history              — показать историю диалога
  settemp <0-1>        — изменить temperature
  setprompt            — задать системный промпт
  showprompt           — показать текущий промпт
  saveprompt           — сохранить промпт в БД
  loadprompt           — загрузить промпт из БД
  listprompts          — список всех промптов
  deleteprompt         — удалить промпт
""")


def run_chat():
    global SYSTEM_PROMPT, temperature

    init_db()

    print("=== Консольный AI ассистент ===")
    print(f"Промпт: {SYSTEM_PROMPT}")
    print(f"Temperature: {temperature}")
    print_help()

    while True:
        user_input = input("Вы: ").strip()
        if not user_input:
            continue

        cmd = user_input.lower()

        if cmd == "exit":
            clear_memory()
            print("Память сессии очищена.")
            print("Выход.")
            break

        if cmd == "history":
            print_history()
            continue

        if cmd.startswith("settemp"):
            try:
                value = float(cmd.split()[1])
                if 0 <= value <= 1:
                    temperature = value
                    print(f"temperature = {temperature}")
                else:
                    print("Значение должно быть в диапазоне от 0 до 1")
            except:
                print("Пример: settemp 0.7")
            continue

        if cmd == "setprompt":
            text = input("Новый системный промпт: ").strip()
            if text:
                SYSTEM_PROMPT = text
                print("Промпт обновлён")
            continue

        if cmd == "showprompt":
            print("\nSYSTEM PROMPT:")
            print(SYSTEM_PROMPT)
            continue

        if cmd == "saveprompt":
            name = input("Название промпта: ").strip()
            if save_prompt(name, SYSTEM_PROMPT):
                print("Промпт сохранён")
            else:
                print("Такое имя уже существует")
            continue

        if cmd == "listprompts":
            prompts = list_prompts()
            if not prompts:
                print("Промптов нет")
                continue
            for pid, name, content in prompts:
                print(f"[{pid}] {name} — {content[:60]}")
            continue

        if cmd == "loadprompt":
            pid = int(input("ID промпта: "))
            text = load_prompt(pid)
            if text:
                SYSTEM_PROMPT = text
                print("Промпт загружен")
            else:
                print("Промпт не найден")
            continue

        if cmd == "deleteprompt":
            pid = int(input("ID промпта: "))
            delete_prompt(pid)
            print("Промпт удалён")
            continue

        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        messages.extend(load_memory())
        messages.append({"role": "user", "content": user_input})

        answer = call_llm(messages, temperature)
        print("AI:", answer)

        save_message("user", user_input)
        save_message("assistant", answer)


if __name__ == "__main__":
    run_chat()
```
## 2. Выполнение заданий и результаты
# Задание 1

Требование: реализовать использование системного промпта через переменную окружения .env, либо через ручной ввод/выбор промпта и его сохранение в базу данных при выборе соответствующей опции в терминале.

Реализация на основе кода:

Системный промпт загружается из переменной окружения SYSTEM_PROMPT (файл .env) с резервным значением по умолчанию:

```python
SYSTEM_PROMPT = os.getenv(
    "SYSTEM_PROMPT",
    "Ты вежливая и хорошая булочка."
)
```

Для хранения промптов используется таблица prompts в SQLite:

```python
conn.execute("""
    CREATE TABLE IF NOT EXISTS prompts (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT UNIQUE NOT NULL,
        content TEXT NOT NULL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
""")
```

Сохранение промпта в БД реализовано через функцию save_prompt, защита от дублирования осуществляется за счёт ограничения UNIQUE по полю name:

```python
def save_prompt(name: str, content: str) -> bool:
    try:
        with get_db() as conn:
            conn.execute(
                "INSERT INTO prompts (name, content) VALUES (?, ?)",
                (name, content)
            )
        return True
    except sqlite3.IntegrityError:
        return False
```

Механизм ручного управления промптами реализован через набор команд в основном цикле:

`setprompt` — установить новый промпт;

`saveprompt` — сохранить текущий промпт в БД;

`loadprompt` — загрузить промпт из БД;

`listprompts` — вывести список доступных промптов;

`deleteprompt` — удалить выбранный промпт.

Результат: системный промпт используется как системное сообщение (role = system) в каждом запросе, что обеспечивает заданный стиль и контекст генерации.

# Задание 2

Требование: поэкспериментировать с параметром temperature в настройках языковой модели, проанализировать поведение языковой модели, результат отразить в отчёте.

Реализация на основе кода:

В программе задано значение `temperature` по умолчанию:

`temperature = 0.7`


Изменение значения temperature осуществляется в интерактивном режиме через команду settemp <0-1>:

```python
if cmd.startswith("settemp"):
    try:
        value = float(cmd.split()[1])
        if 0 <= value <= 1:
            temperature = value
            print(f"temperature = {temperature}")
        else:
            print("Значение должно быть в диапазоне от 0 до 1")
    except:
        print("Пример: settemp 0.7")
    continue
```

Параметр temperature передаётся в API-запрос:

```python
payload = {
    "model": MODEL_NAME,
    "messages": messages,
    "temperature": temp
}
```

Анализ поведения модели:

При увеличении `temperature` модель генерирует более вариативные и креативные ответы (возрастает доля вероятностного выбора токенов).

При уменьшении `temperature` ответы становятся более предсказуемыми и детерминированными.

Задание 3

Требование: реализовать ведение истории диалога (контекста переписки с ассистентом), чтобы ИИ учитывал предыдущие сообщения. Длину истории сообщений ограничить до 6 последних сообщений (3 пользовательских, 3 ассистентских).

Реализация на основе кода:

Ограничение длины истории задаётся константой:

`MAX_MEMORY = 6`


Для хранения истории создана таблица chat_memory (SQLite):

```python
conn.execute("""
    CREATE TABLE IF NOT EXISTS chat_memory (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        role TEXT NOT NULL,
        content TEXT NOT NULL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
""")
```

При сохранении каждого сообщения выполняется автоматическое удаление старых записей, если превышен лимит MAX_MEMORY:

```python
def save_message(role: str, content: str):
    with get_db() as conn:
        conn.execute(
            "INSERT INTO chat_memory (role, content) VALUES (?, ?)",
            (role, content)
        )

        conn.execute(f"""
            DELETE FROM chat_memory
            WHERE id NOT IN (
                SELECT id FROM chat_memory
                ORDER BY created_at DESC
                LIMIT {MAX_MEMORY}
            )
        """)
```

История подгружается и добавляется к сообщениям запроса модели:

```python
messages = [{"role": "system", "content": SYSTEM_PROMPT}]
messages.extend(load_memory())
messages.append({"role": "user", "content": user_input})

```
Результат: модель получает ограниченный контекст последних сообщений, что позволяет учитывать актуальную часть диалога и при этом контролировать размер контекста.

## 3. Итог

В ходе выполнения лабораторной работы:

Реализовано использование системного промпта через .env и механизм сохранения/загрузки промптов в SQLite;

Реализовано управление параметром temperature и проанализировано его влияние на характер генерации;

Реализовано хранение истории диалога с ограничением до 6 последних сообщений (3 пользовательских, 3 ассистентских).

Все требования лабораторной работы выполнены.



